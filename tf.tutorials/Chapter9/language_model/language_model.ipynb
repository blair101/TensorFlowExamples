{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 完整的训练语言模型\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DATA = 'ptb.train'\n",
    "TEST_DATA = 'ptb.test'\n",
    "EVAL_DATA = 'ptb.valid'\n",
    "HIDDEN_SIZE  = 300\n",
    "NUM_LAYERS = 2 # LSTM 结构的层数\n",
    "VOCAB_SIZE = 10000 # 字典规模\n",
    "TRAIN_BATCH_SIZE = 20 # 训练数据batch大小\n",
    "TRAIN_NUM_STEP = 35 # 训练数据阶段长度\n",
    "\n",
    "EVAL_BATCH_SIZE = 1 # 测试数据batch大小\n",
    "EVAL_NUM_STEP = 1 # 测试数据阶段长度\n",
    "NUM_EPOCH = 5 # 使用训练数据轮数\n",
    "LSTM_KEEP_PROB = 0.9\n",
    "EMBEDDING_KEEP_PROB = 0.9\n",
    "MAX_GRAD_NORM = 5 # 用于控制梯度膨胀的梯度大小上限\n",
    "SHARED_EMB_AND_SOFTMAX = True # 在softmax层和词向量层之间共享参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 通过一个PTBModel类来描述模型\n",
    "class PTBModel(object):\n",
    "    def __init__(self, is_training, batch_size, num_steps):\n",
    "        '''\n",
    "        Arg:\n",
    "            is_training: 表示是否在训练\n",
    "            batch_size: 表示batch size\n",
    "            num_stpes: 表示截断长度\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.num_steps = num_steps\n",
    "        # shape (batch_size, num_steps)\n",
    "        self.input_data = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        self.targets = tf.placeholder(tf.int32, [batch_size, num_steps])\n",
    "        \n",
    "        # 构造LSTM结构，多层的LSTM，包括了dropout机制\n",
    "        dropout_keep_prob = LSTM_KEEP_PROB if is_training else 1.0\n",
    "        lstm_cells = [\n",
    "            tf.nn.rnn_cell.DropoutWrapper(\n",
    "            tf.nn.rnn_cell.BasicLSTMCell(HIDDEN_SIZE),\n",
    "                output_keep_prob = dropout_keep_prob\n",
    "            ) for _ in range(NUM_LAYERS)\n",
    "        ]\n",
    "        # 多层结构\n",
    "        cell = tf.nn.rnn_cell.MultiRNNCell(lstm_cells)\n",
    "        \n",
    "        # 表示初始状态\n",
    "        self.initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "        \n",
    "        # 定义单词的词向量矩阵\n",
    "        embedding = tf.get_variable(\"embedding\", [VOCAB_SIZE, HIDDEN_SIZE]) # 变量\n",
    "        \n",
    "        # 将数据转化为词向量表示 [batch_size, num_step, HIDDEN_SIZE]\n",
    "        inputs = tf.nn.embedding_lookup(embedding, self.input_data)\n",
    "        \n",
    "        # 只在训练时使用dropout\n",
    "        if is_training:\n",
    "            inputs = tf.nn.dropout(inputs, EMBEDDING_KEEP_PROB)\n",
    "        \n",
    "        # 定义输出列表，先将不同时刻LSTM结构的输出收集起来，再一起提供给softmax层\n",
    "        outputs = []\n",
    "        state = self.initial_state\n",
    "        with tf.variable_scope('RNN'):\n",
    "            for time_step in range(num_steps):\n",
    "                if time_step > 0:\n",
    "                    tf.get_variable_scope().reuse_variables()\n",
    "                cell_output, state = cell(inputs[:,time_step,:], state) # 每次计算每个time_step的结果\n",
    "                outputs.append(cell_output)\n",
    "                \n",
    "        # 把输出队列展开成[batch, hidden_size * num_steps]的形状，然后再reshape成(batch * num_steps, hidden_size)\n",
    "        output = tf.reshape(tf.concat(outputs, 1),[-1, HIDDEN_SIZE])\n",
    "        \n",
    "        # softmax层：将RNN在每个位置上的输出转化为各个单词的logits\n",
    "        if SHARED_EMB_AND_SOFTMAX:\n",
    "            weight = tf.transpose(embedding)\n",
    "        else:\n",
    "            weight = tf.get_variable(\"weight\", [HIDDEN_SIZE, VOCAB_SIZE])\n",
    "            \n",
    "        bias = tf.get_variable(\"bias\", [VOCAB_SIZE])\n",
    "        # shape (batch * num_steps, vocab_size)\n",
    "        logits = tf.matmul(output, weight) + bias\n",
    "        \n",
    "        loss = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            labels = tf.reshape(self.targets, [-1]),\n",
    "            # shape (batch_size * num_steps,)\n",
    "            logits = logits\n",
    "        )\n",
    "        self.cost = tf.reduce_sum(loss) / batch_size\n",
    "        self.final_state = state\n",
    "        \n",
    "        # 如果是训练状态，那么还需要实现反向传播\n",
    "        if not is_training:\n",
    "            return\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1.0)\n",
    "        self.train_op = optimizer.minimize(self.cost)\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # 这个训练过程没有学过\n",
    "        trainable_variables = tf.trainable_variables() # 获取到模型中所有需要训练的变量\n",
    "        grads, _ = tf.clip_by_global_norm(tf.gradients(self.cost, trainable_variables), MAX_GRAD_NORM) # 求导，并且进行截断\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate = 1.0)\n",
    "        self.train_op = optimizer.apply_gradients(\n",
    "            zip(grads, trainable_variables)\n",
    "        )\n",
    "        '''\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_epoch(session, model, batches, train_op, output_log, step):\n",
    "    '''\n",
    "    训练函数\n",
    "    Args:\n",
    "        session 上下文\n",
    "        model 上述模型的实例\n",
    "        batches 数据\n",
    "        train_op\n",
    "        output_log 是否打印输出日志\n",
    "        step\n",
    "    Return:\n",
    "        step\n",
    "        结果\n",
    "    '''\n",
    "    total_costs = 0.0\n",
    "    iters = 0\n",
    "    state = session.run(model.initial_state)\n",
    "    \n",
    "    # 训练一个epoch\n",
    "    for x,y in batches:\n",
    "        # 在当前batch上运行train_op 并计算损失值\n",
    "        cost, state, _ = session.run([model.cost, model.final_state, train_op],\n",
    "                                  feed_dict = {model.input_data: x, model.targets: y, model.initial_state: state}\n",
    "                                 )\n",
    "        total_costs += cost\n",
    "        iters += model.num_steps # 迭代次数\n",
    "        \n",
    "        # 在训练时输出日志\n",
    "        if output_log and step % 100 == 0:\n",
    "            print (\"After %d steps, perplexity is % .3f\" % (step, np.exp(total_costs / iters)))\n",
    "        step += 1\n",
    "    \n",
    "    return step, np.exp(total_costs / iters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_data(file_path):\n",
    "    '''\n",
    "    读取数据，返回包含单词编号的数组，一整个文本的内容作为一个数组返回，每行句子拼接起来\n",
    "    '''\n",
    "    with open(file_path, 'r') as fin:\n",
    "        id_string = ' '.join([line.strip() for line in fin.readlines()])\n",
    "    id_list = [int(w) for w in id_string.split()]\n",
    "    return id_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "Missing parentheses in call to 'print'. Did you mean print(int num_batches)? (<ipython-input-7-2015a256f5b5>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-7-2015a256f5b5>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    print num_batches\u001b[0m\n\u001b[0m                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m Missing parentheses in call to 'print'. Did you mean print(int num_batches)?\n"
     ]
    }
   ],
   "source": [
    "def make_batches(id_list, batch_size, num_step):\n",
    "    '''\n",
    "    获取到batch\n",
    "    Args:\n",
    "        id_list: 一整个文本组成的数组，内容是word的id\n",
    "        batch_size: batch的大小\n",
    "        num_step: 表示训练时的上下文，输入的单词个数\n",
    "    '''\n",
    "    num_batches = (len(id_list) - 1) // (batch_size * num_step) # batch的数量\n",
    "    print num_batches\n",
    "    data = np.array(id_list[:num_batches * batch_size * num_step])\n",
    "    print data.shape\n",
    "    data = np.reshape(data, [batch_size, num_batches * num_step]) # 将数据切分成 batch_size, num_batches * num_steps的数组\n",
    "    # 沿着第二个维度将数据切分为num_batches的batch，存入一个数组\n",
    "    print data.shape\n",
    "    data_batches = np.split(data, num_batches, axis = 1)\n",
    "    print data_batches[0].shape\n",
    "    \n",
    "    label = np.array(id_list[1:num_batches * batch_size * num_step + 1])\n",
    "    label = np.reshape(label, [batch_size, num_batches * num_step])\n",
    "    label_batches = np.split(label, num_batches, axis=1)\n",
    "    return list(zip(data_batches, label_batches))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    initializer = tf.random_uniform_initializer(-0.05, 0.05) # 初始化函数\n",
    "    \n",
    "    # 训练的RNN模型\n",
    "    with tf.variable_scope(\"language_model\", reuse=None, initializer=initializer):\n",
    "        train_model = PTBModel(is_training=True, batch_size=TRAIN_BATCH_SIZE, num_steps=TRAIN_NUM_STEP)\n",
    "    \n",
    "    \n",
    "    # 测试用的模型，与train_model共用参数，但没有dropout(is_training=False)\n",
    "    with tf.variable_scope(\"language_model\", reuse=True, initializer=initializer):\n",
    "        eval_model = PTBModel(is_training=False, batch_size=EVAL_BATCH_SIZE, num_steps=EVAL_NUM_STEP)\n",
    "    \n",
    "    # 训练模型\n",
    "    with tf.Session() as sess:\n",
    "        # 初始化\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        # 训练数据\n",
    "        train_batches = make_batches(\n",
    "            read_data(TRAIN_DATA),\n",
    "            TRAIN_BATCH_SIZE,\n",
    "            TRAIN_NUM_STEP\n",
    "        )\n",
    "        \n",
    "        # eval数据\n",
    "        eval_batches = make_batches(\n",
    "            read_data(EVAL_DATA),\n",
    "            EVAL_BATCH_SIZE,\n",
    "            EVAL_NUM_STEP\n",
    "        )\n",
    "        \n",
    "        # test数据\n",
    "        test_batches = make_batches(\n",
    "            read_data(TEST_DATA),\n",
    "            EVAL_BATCH_SIZE,\n",
    "            EVAL_NUM_STEP\n",
    "        )\n",
    "        \n",
    "        step = 0\n",
    "        # 每一轮\n",
    "        for i in range(NUM_EPOCH):\n",
    "            print (\"In iteration: %d\" % (i + 1))\n",
    "            # 训练过程\n",
    "            step, train_pplx = run_epoch(sess, train_model, train_batches, \n",
    "                                         train_model.train_op, \n",
    "                                         output_log = True,\n",
    "                                         step = step\n",
    "                                        )\n",
    "            print (\"Epoch: %d Train Perplexity: %.3f\" % (i+1, train_pplx))\n",
    "            \n",
    "            # evaluation过程\n",
    "            _, eval_pplx = run_epoch(sess, eval_model, eval_batches, \n",
    "                                     tf.no_op(), \n",
    "                                     output_log = False,\n",
    "                                     step = 0\n",
    "                                    )\n",
    "            print (\"Epoch: %d Eval Perplexity: %.3f\" % (i + 1, eval_pplx))\n",
    "        _, test_pplx = run_epoch(sess, eval_model, eval_batches, tf.no_op(),output_log = False, step = 0)\n",
    "        # 训练结束进行test\n",
    "        print (\"Test Perplexity: %.3f\" % (test_pplx))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1327\n",
      "(928900,)\n",
      "(20, 46445)\n",
      "(20, 35)\n",
      "73759\n",
      "(73759,)\n",
      "(1, 73759)\n",
      "(1, 1)\n",
      "82429\n",
      "(82429,)\n",
      "(1, 82429)\n",
      "(1, 1)\n",
      "In iteration: 1\n",
      "After 0 steps, perplexity is  10010.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/apple/Library/Python/2.7/lib/python/site-packages/ipykernel_launcher.py:30: RuntimeWarning: overflow encountered in exp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 100 steps, perplexity is  inf\n",
      "After 200 steps, perplexity is  inf\n",
      "After 300 steps, perplexity is  inf\n",
      "After 400 steps, perplexity is  inf\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-263240bbee7e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-196ec0e6aa1d>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     45\u001b[0m                                          \u001b[0mtrain_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_op\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                                          \u001b[0moutput_log\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                                          \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                                         )\n\u001b[1;32m     49\u001b[0m             \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Epoch: %d Train Perplexity: %.3f\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_pplx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-4d987790cf88>\u001b[0m in \u001b[0;36mrun_epoch\u001b[0;34m(session, model, batches, train_op, output_log, step)\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;31m# 在当前batch上运行train_op 并计算损失值\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         cost, state, _ = session.run([model.cost, model.final_state, train_op],\n\u001b[0;32m---> 23\u001b[0;31m                                   \u001b[0mfeed_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_data\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitial_state\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m                                  )\n\u001b[1;32m     25\u001b[0m         \u001b[0mtotal_costs\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcost\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    903\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    904\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 905\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    906\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1135\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1136\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1137\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1138\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1139\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1354\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1355\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1356\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1357\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1359\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1360\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1361\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1362\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1363\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Python/2.7/site-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1338\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1339\u001b[0m           return tf_session.TF_Run(session, options, feed_dict, fetch_list,\n\u001b[0;32m-> 1340\u001b[0;31m                                    target_list, status, run_metadata)\n\u001b[0m\u001b[1;32m   1341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1342\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
